In [2]: run src/models/model_score_overtime.py
Optimization terminated successfully.
         Current function value: 0.678370
         Iterations 7
                           Logit Regression Results
==============================================================================
Dep. Variable:                      y   No. Observations:                 1411
Model:                          Logit   Df Residuals:                     1401
Method:                           MLE   Df Model:                            9
Date:                Tue, 23 May 2017   Pseudo R-squ.:                -0.08080
Time:                        08:22:46   Log-Likelihood:                -957.18
converged:                       True   LL-Null:                       -885.62
                                        LLR p-value:                     1.000
======================================================================================
                         coef    std err          z      P>|z|      [0.025      0.975]
--------------------------------------------------------------------------------------
n_logins              -0.3773      0.188     -2.012      0.044      -0.745      -0.010
n_viewed               1.0430      0.388      2.690      0.007       0.283       1.803
n_posts_created       -0.4309      0.240     -1.792      0.073      -0.902       0.041
last_login            -0.1427      0.059     -2.429      0.015      -0.258      -0.028
n_msg_sent             0.2817      0.221      1.272      0.203      -0.152       0.716
n_prior_enrollment    -0.1267      0.193     -0.655      0.512      -0.506       0.252
0_forum               -0.0728      0.081     -0.903      0.366      -0.231       0.085
0_page_1               0.0996      0.098      1.012      0.312      -0.093       0.293
0_page_2               0.0356      0.091      0.393      0.694      -0.142       0.213
0_page_3               0.1457      0.083      1.758      0.079      -0.017       0.308
======================================================================================
MF df ~ y
{'penalty': 'l1', 'C': 0.1, 'class_weight': 'balanced'}
             precision    recall  f1-score   support

      False       0.69      0.78      0.74       192
       True       0.37      0.27      0.32        91

avg / total       0.59      0.62      0.60       283

{'max_leaf_nodes': 3, 'max_depth': 3, 'class_weight': 'balanced'}
             precision    recall  f1-score   support

      False       0.72      0.58      0.64       192
       True       0.37      0.52      0.43        91

avg / total       0.60      0.56      0.57       283

{'kernel': 'rbf', 'C': 0.1, 'class_weight': 'balanced'}
             precision    recall  f1-score   support

      False       0.68      0.81      0.74       192
       True       0.33      0.20      0.25        91

avg / total       0.57      0.61      0.58       283

Optimization terminated successfully.
         Current function value: 0.530160
         Iterations 7
                           Logit Regression Results
==============================================================================
Dep. Variable:                      y   No. Observations:                 1799
Model:                          Logit   Df Residuals:                     1781
Method:                           MLE   Df Model:                           17
Date:                Tue, 23 May 2017   Pseudo R-squ.:                  0.1388
Time:                        08:22:52   Log-Likelihood:                -953.76
converged:                       True   LL-Null:                       -1107.5
                                        LLR p-value:                 3.110e-55
======================================================================================
                         coef    std err          z      P>|z|      [0.025      0.975]
--------------------------------------------------------------------------------------
n_logins               0.2099      0.118      1.780      0.075      -0.021       0.441
n_viewed               1.2509      0.288      4.338      0.000       0.686       1.816
n_posts_created        0.0517      0.234      0.221      0.825      -0.406       0.509
last_login            -0.2054      0.060     -3.411      0.001      -0.323      -0.087
n_msg_sent             0.1168      0.195      0.600      0.549      -0.265       0.499
n_prior_enrollment     0.0496      0.096      0.517      0.605      -0.138       0.237
0_forum                0.0471      0.084      0.563      0.574      -0.117       0.211
0_page_1               0.0184      0.113      0.164      0.870      -0.202       0.239
0_page_2              -0.1039      0.114     -0.908      0.364      -0.328       0.120
0_page_3               0.2077      0.100      2.077      0.038       0.012       0.404
1_forum                0.1802      0.080      2.259      0.024       0.024       0.337
1_book_1               0.2979      0.125      2.375      0.018       0.052       0.544
1_book_2              -0.1438      0.131     -1.094      0.274      -0.401       0.114
1_quiz                 0.1897      0.098      1.935      0.053      -0.002       0.382
1_feedback             0.3316      0.095      3.490      0.000       0.145       0.518
1_choice              -0.1496      0.088     -1.698      0.089      -0.322       0.023
1_page_2               0.0212      0.107      0.199      0.842      -0.188       0.230
1_page_1               0.0067      0.077      0.087      0.931      -0.145       0.158
======================================================================================
MF df ~ y
{'penalty': 'l2', 'C': 0.01, 'class_weight': 'balanced'}
             precision    recall  f1-score   support

      False       0.91      0.85      0.88       250
       True       0.71      0.82      0.76       110

avg / total       0.85      0.84      0.84       360

{'max_leaf_nodes': None, 'max_depth': 3, 'class_weight': 'balanced'}
             precision    recall  f1-score   support

      False       0.91      0.79      0.85       250
       True       0.64      0.83      0.72       110

avg / total       0.83      0.80      0.81       360

{'kernel': 'rbf', 'C': 0.1, 'class_weight': 'balanced'}
             precision    recall  f1-score   support

      False       0.90      0.84      0.87       250
       True       0.69      0.79      0.73       110

avg / total       0.84      0.82      0.83       360

Optimization terminated successfully.
         Current function value: 0.419468
         Iterations 7
                           Logit Regression Results
==============================================================================
Dep. Variable:                      y   No. Observations:                 1997
Model:                          Logit   Df Residuals:                     1973
Method:                           MLE   Df Model:                           23
Date:                Tue, 23 May 2017   Pseudo R-squ.:                  0.3099
Time:                        08:23:01   Log-Likelihood:                -837.68
converged:                       True   LL-Null:                       -1213.8
                                        LLR p-value:                4.185e-144
======================================================================================
                         coef    std err          z      P>|z|      [0.025      0.975]
--------------------------------------------------------------------------------------
n_logins               0.3834      0.143      2.684      0.007       0.103       0.663
n_viewed               0.5418      0.227      2.392      0.017       0.098       0.986
n_posts_created        0.3990      0.392      1.019      0.308      -0.368       1.166
last_login            -0.1677      0.079     -2.121      0.034      -0.323      -0.013
n_msg_sent             0.2203      0.301      0.732      0.464      -0.369       0.810
n_prior_enrollment    -0.0075      0.102     -0.074      0.941      -0.206       0.191
0_forum                0.0181      0.094      0.192      0.848      -0.167       0.203
0_page_1              -0.0174      0.118     -0.147      0.883      -0.249       0.214
0_page_2              -0.0581      0.116     -0.499      0.618      -0.286       0.170
0_page_3               0.1492      0.094      1.584      0.113      -0.035       0.334
1_forum                0.0860      0.087      0.985      0.325      -0.085       0.257
1_book_1               0.1821      0.120      1.523      0.128      -0.052       0.417
1_book_2              -0.1130      0.146     -0.777      0.437      -0.398       0.172
1_quiz                 0.2127      0.115      1.846      0.065      -0.013       0.439
1_feedback             0.3078      0.116      2.663      0.008       0.081       0.534
1_choice              -0.1696      0.096     -1.765      0.078      -0.358       0.019
1_page_2              -0.0067      0.132     -0.051      0.959      -0.265       0.252
1_page_1              -0.1332      0.078     -1.712      0.087      -0.286       0.019
2_quiz                 0.4721      0.119      3.979      0.000       0.240       0.705
2_book                -0.1593      0.115     -1.382      0.167      -0.385       0.067
2_glossary             0.3189      0.148      2.155      0.031       0.029       0.609
2_survey               0.1079      0.238      0.454      0.650      -0.358       0.574
2_wiki                 0.6953      0.241      2.882      0.004       0.223       1.168
2_page_1               0.0013      0.098      0.013      0.990      -0.190       0.193
======================================================================================
MF df ~ y
{'penalty': 'l1', 'C': 0.1, 'class_weight': 'balanced'}
             precision    recall  f1-score   support

      False       0.95      0.86      0.91       281
       True       0.74      0.90      0.81       119

avg / total       0.89      0.88      0.88       400

{'max_leaf_nodes': 20, 'max_depth': 4, 'class_weight': 'balanced'}
             precision    recall  f1-score   support

      False       0.96      0.81      0.88       281
       True       0.67      0.92      0.78       119

avg / total       0.88      0.84      0.85       400

{'kernel': 'rbf', 'C': 0.1, 'class_weight': 'balanced'}
             precision    recall  f1-score   support

      False       0.98      0.85      0.91       281
       True       0.73      0.95      0.82       119

avg / total       0.90      0.88      0.88       400

Optimization terminated successfully.
         Current function value: 0.282894
         Iterations 8
                           Logit Regression Results
==============================================================================
Dep. Variable:                      y   No. Observations:                 2130
Model:                          Logit   Df Residuals:                     2100
Method:                           MLE   Df Model:                           29
Date:                Tue, 23 May 2017   Pseudo R-squ.:                  0.5303
Time:                        08:23:12   Log-Likelihood:                -602.56
converged:                       True   LL-Null:                       -1282.9
                                        LLR p-value:                2.761e-268
======================================================================================
                         coef    std err          z      P>|z|      [0.025      0.975]
--------------------------------------------------------------------------------------
n_logins               0.6791      0.194      3.498      0.000       0.299       1.060
n_viewed               0.4724      0.206      2.291      0.022       0.068       0.877
n_posts_created        2.3156      0.721      3.210      0.001       0.902       3.729
last_login            -0.1240      0.114     -1.085      0.278      -0.348       0.100
n_msg_sent            -0.7332      0.510     -1.439      0.150      -1.732       0.265
n_prior_enrollment     0.0527      0.125      0.421      0.673      -0.192       0.298
0_forum               -0.0244      0.116     -0.210      0.834      -0.253       0.204
0_page_1              -0.0611      0.161     -0.379      0.705      -0.377       0.255
0_page_2              -0.0298      0.155     -0.193      0.847      -0.333       0.273
0_page_3               0.1626      0.118      1.381      0.167      -0.068       0.393
1_forum               -0.2203      0.115     -1.916      0.055      -0.446       0.005
1_book_1              -0.0895      0.152     -0.588      0.556      -0.388       0.209
1_book_2              -0.0495      0.174     -0.285      0.776      -0.390       0.291
1_quiz                 0.0197      0.163      0.121      0.904      -0.300       0.340
1_feedback             0.2555      0.154      1.659      0.097      -0.046       0.557
1_choice              -0.0980      0.124     -0.792      0.428      -0.341       0.145
1_page_2              -0.0785      0.163     -0.483      0.629      -0.397       0.240
1_page_1               0.0022      0.101      0.022      0.983      -0.197       0.201
2_quiz                 0.3487      0.146      2.388      0.017       0.063       0.635
2_book                -0.1665      0.159     -1.050      0.294      -0.477       0.144
2_glossary             0.5034      0.152      3.310      0.001       0.205       0.802
2_survey               0.0275      0.207      0.133      0.894      -0.378       0.433
2_wiki                 0.4222      0.189      2.235      0.025       0.052       0.792
2_page_1              -0.1180      0.148     -0.800      0.424      -0.407       0.171
3_quiz                 0.6583      0.156      4.221      0.000       0.353       0.964
3_book                -0.1175      0.160     -0.734      0.463      -0.431       0.196
3_forum                1.2086      0.227      5.313      0.000       0.763       1.654
3_data                -0.0188      0.126     -0.149      0.882      -0.266       0.228
3_page_1               0.1976      0.111      1.782      0.075      -0.020       0.415
3_page_2               0.2748      0.199      1.378      0.168      -0.116       0.666
======================================================================================
MF df ~ y
{'penalty': 'l2', 'C': 0.01, 'class_weight': 'balanced'}
             precision    recall  f1-score   support

      False       0.99      0.89      0.94       302
       True       0.79      0.97      0.87       124

avg / total       0.93      0.92      0.92       426

{'max_leaf_nodes': 10, 'max_depth': 3, 'class_weight': 'balanced'}
             precision    recall  f1-score   support

      False       0.98      0.89      0.93       302
       True       0.78      0.96      0.86       124

avg / total       0.92      0.91      0.91       426

{'kernel': 'rbf', 'C': 0.001, 'class_weight': 'balanced'}
             precision    recall  f1-score   support

      False       0.71      1.00      0.83       302
       True       0.00      0.00      0.00       124

avg / total       0.50      0.71      0.59       426
